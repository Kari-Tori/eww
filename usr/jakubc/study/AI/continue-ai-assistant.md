---
version: 0.0.1
modified: 2025-11-25
title: Continue - AI Coding Assistant
category: AI Tools
type: IDE Extension
status: Recommended
cost: FREE
ollama_support: YES
created: 2025-11-25
---

# Continue - AI Coding Assistant

## Podstawowe informacje

- **Typ:** Interactive coding assistant
- **Integracja:** VS Code / JetBrains
- **Licencja:** Apache 2.0 (Open Source)
- **Koszt:** FREE
- **Repozytorium:** https://github.com/continuedev/continue
- **Dokumentacja:** https://docs.continue.dev

## G≈Ç√≥wne funkcje

### 1. Tab Autocomplete
- Automatyczne uzupe≈Çnianie kodu podczas pisania
- Kontekstowe sugestie
- Dzia≈Ça jak GitHub Copilot, ale lokalnie

### 2. Chat w IDE
- Pytania o kod w oknie VS Code
- Wyja≈õnienia funkcji
- Code review w czasie rzeczywistym
- Sugestie refaktoringu

### 3. Code Actions
- "Napisz tƒô funkcjƒô"
- "Dodaj testy"
- "Dokumentuj to"
- "Znajd≈∫ b≈ÇƒÖd"

## Konfiguracja z Ollama

### Moja gotowa konfiguracja
Lokalizacja: `/home/karinam/git/eww/dev/cfg/continue-config.json`

```json
{
  "models": [
    {
      "title": "Qwen 2.5 Coder 7B (Remote)",
      "provider": "ollama",
      "model": "qwen2.5-coder:7b",
      "apiBase": "http://localhost:11434"
    },
    {
      "title": "Llama 3 8B (Remote)",
      "provider": "ollama",
      "model": "llama3:8b",
      "apiBase": "http://localhost:11434"
    }
  ],
  "tabAutocompleteModel": {
    "title": "Qwen Autocomplete",
    "provider": "ollama",
    "model": "qwen2.5-coder:7b",
    "apiBase": "http://localhost:11434"
  },
  "embeddingsProvider": {
    "provider": "ollama",
    "model": "nomic-embed-text",
    "apiBase": "http://localhost:11434"
  }
}
```

## Instalacja

### Krok 1: Instalacja rozszerzenia
```bash
code --install-extension continue.continue
```

### Krok 2: Konfiguracja
```bash
# Stw√≥rz katalog konfiguracyjny
mkdir -p ~/.continue

# Skopiuj gotowƒÖ konfiguracjƒô
cp /home/karinam/git/eww/dev/cfg/continue-config.json ~/.continue/config.json
```

### Krok 3: Sprawd≈∫ Ollama
```bash
# Sprawd≈∫ czy Ollama dzia≈Ça
curl http://localhost:11434/api/tags

# Pobierz potrzebne modele (je≈õli nie masz)
ollama pull qwen2.5-coder:7b
ollama pull llama3:8b
ollama pull nomic-embed-text
```

### Krok 4: Restart VS Code
- Zamknij i otw√≥rz VS Code
- Continue powinno byƒá aktywne w pasku bocznym

## U≈ºycie

### Autocomplete
- Po prostu pisz kod
- Continue bƒôdzie podpowiadaƒá w szarym kolorze
- `Tab` aby zaakceptowaƒá

### Chat
- Otw√≥rz panel Continue (ikona w pasku bocznym)
- Zadawaj pytania o kod
- Zaznacz kod ‚Üí kliknij prawym ‚Üí "Continue: ..."

### Komendy
- `Ctrl+L` - Otw√≥rz chat
- `Ctrl+I` - Inline edit
- `Ctrl+Shift+R` - Refactor selection

## Zalety

‚úÖ **Ca≈Çkowicie darmowe** - licencja open source  
‚úÖ **Lokalnie z Ollama** - zero koszt√≥w API, prywatno≈õƒá  
‚úÖ **Wbudowane w VS Code** - tam gdzie pracujesz  
‚úÖ **Tab autocomplete** - productivity boost  
‚úÖ **Prosty setup** - 5 minut  
‚úÖ **Kontrola** - Ty decydujesz o zmianach

## Wady

‚ö†Ô∏è **Nie autonomiczny** - asystent, nie agent  
‚ö†Ô∏è **Wymaga interakcji** - musisz akceptowaƒá sugestie  
‚ö†Ô∏è **Lokalne modele** - wolniejsze ni≈º GPT-4

## Przypadki u≈ºycia

### Idealne dla:
- Codzienne kodowanie z asystentem
- Quick questions o kod
- Code explanations
- Refactoring suggestions
- Writing tests
- Documentation

### NIE idealne dla:
- Autonomiczne wykonywanie ca≈Çych task√≥w
- "Zr√≥b to za mnie od A do Z"
- Du≈ºe refaktoringi bez nadzoru

## Por√≥wnanie z alternatywami

| Funkcja | Continue | GitHub Copilot | Cursor |
|---------|----------|----------------|--------|
| Koszt | FREE | $10-19/m | $20/m |
| Ollama | ‚úÖ | ‚ùå | ‚ùå |
| Lokalne | ‚úÖ | ‚ùå | Czƒô≈õciowo |
| VS Code | ‚úÖ | ‚úÖ | W≈Çasny editor |
| Open Source | ‚úÖ | ‚ùå | ‚ùå |

## Podsumowanie

**Continue = najlepszy wyb√≥r dla lokalnego AI coding assistant**

- Darmowy i open source
- Dzia≈Ça z mojƒÖ lokalnƒÖ OllamƒÖ
- Prosty w setupie
- Produktywno≈õƒá od zaraz
- Pe≈Çna kontrola nad kodem

## Status
‚úÖ **Gotowe do u≈ºycia** - mam ju≈º config w `/git/eww/dev/cfg/`

## Limity pamiƒôci (zapobiega OOM w VS Code)

### Problem
Continue + Ollama mogƒÖ zu≈ºywaƒá za du≈ºo RAM ‚Üí VS Code crashuje (OOM)

### RozwiƒÖzanie
Konfiguracja limit√≥w w `/etc/systemd/system/ollama.service.d/override.conf`:

```ini
[Service]
Environment="OLLAMA_MAX_LOADED_MODELS=1"    # Tylko 1 model w VRAM
Environment="OLLAMA_NUM_PARALLEL=1"          # 1 request na raz
Environment="OLLAMA_MAX_QUEUE=2"             # Max 2 w kolejce
Environment="OLLAMA_KEEP_ALIVE=2m"           # Zwalniaj po 2 min
```

**Restart po zmianach:**
```bash
sudo systemctl daemon-reload
sudo systemctl restart ollama
```

### Efekt
- RAM: ~2-3 GB mniej u≈ºywane
- VRAM: tylko 1 model naraz (swap gdy prze≈ÇƒÖczasz)
- VS Code: stabilny, bez crashy

## Nastƒôpne kroki
1. Install extension
2. Copy config
3. Verify Ollama
4. Configure limits (je≈õli OOM)
5. Start coding!

## üîó Backlinks

- [[usr/jakubc/study/AI/AI]]
- [[usr/jakubc/jakubc]]
- [[EWW-MAP]]