---
tags:
  - #automation
  - #development
  - #eww
  - #jakubc
  - #knowledge
  - #linux
  - #secondbrain
created: 2025-11-21T17:00:00Z
modified: 2025-11-21T17:00:00Z
author: jakubc
title: "ğŸ§  EMBEDDINGS AI / SECOND-BRAIN INDEX"






---
---
title: ğŸ§  EMBEDDINGS AI / SECOND-BRAIN INDEX
  - smart-connections
  - embeddings
  - obsidian
  - ai-index
  - retrieval
  - rag
  - personal
  - blue
  - jakubc
status: draft
updated: 2025-11-21
aliases:
  - ğŸ§  EMBEDDINGS AI / SECOND-BRAIN INDEX
linter-yaml-title-alias: ğŸ§  EMBEDDINGS AI / SECOND-BRAIN INDEX
date created: 2025-11-21
date modified: poniedziaÅ‚ek, paÅºdziernik 27. 2025, 9:00:56 pm
layout: article
icon: ğŸ§ 
"group:": |-
  âš¡ Smart Connections 
  ğŸš€ High Performance
focus: |-
  semantyczne linkowanie
  wyszukiwanie po znaczeniu
  RAG
secondbrain: "true"
module: embeddings
dv_ready: "true"
color: blue
---

# ğŸ§  EMBEDDINGS AI / SECOND-BRAIN INDEX

> [!summary] ğŸ› Funkcja
> Embedding = zamiana tekstu w wektor liczb â†’ wektor moÅ¼na porÃ³wnywaÄ‡ â†’ "to jest podobne do tamtego".
>
> UÅ¼ycie w Secondbrain:
> - ğŸ”— automatyczne linkowanie notatek
> - ğŸ” wyszukiwanie semantyczne ("znajdÅº podobny SOP / zgÅ‚oszenie / objaw")
> - ğŸ§© RAG
>
> Dwie klasy modeli:
> - âš¡ Smart Connections â†’ ğŸ’» lekkie, lokalne, na laptopie
> - ğŸš€ High Performance â†’ ğŸ­ ciÄ™Å¼sze, 8K kontekst, PL+EN, centralna baza wiedzy

KaÅ¼dy model ma staÅ‚e pola do Dataview:
- `model::`
- `class::` (smart-connections / high-performance)
- `role::` (block_index / note_index / writing-assist)
- `dim::`
- `context_tokens::`
- `params_m::`
- `lang::`
- `multilang::` (true/false)
- `ram_fp16_mb::`
- `cpu_min_threads::`
- `cpu_rec_threads::`
- `gpu_required::` (none / optional / required / recommended)
- `host_ram_gb_min::`
- `tier::` (light / medium / heavy)
- `repo::`

To pozwala filtrowaÄ‡ modele z poziomu Obsidiana.

---

## 0. ğŸ— ARCHITEKTURA SECOND-BRAIN

### 0.1 ğŸ§± Block index vs ğŸ“„ Note index

ğŸ§± **Block index (FRAGMENTY)**  
- Jednostka: ğŸ”© akapit, âœ… checklist, ğŸ”§ krok naprawy, ğŸ« ticket serwisowy  
- Modele krÃ³tkiego kontekstu (~512 tokenÃ³w)  
- Niski koszt, szybka inferencja  
- Wektor: **384-dim**

ğŸ“„ **Note index (CAÅE PLIKI)**  
- Jednostka: ğŸ“„ peÅ‚na notatka `.md`  
  (ğŸ“˜ SOP, ğŸ“ intake, ğŸ›  raport serwisowy, ğŸ”¥ post-mortem)  
- Modele dÅ‚ugiego kontekstu (2K / 4K / 8K tokenÃ³w)  
- MajÄ… zÅ‚apaÄ‡ sens caÅ‚ego dokumentu jako jednej caÅ‚oÅ›ci  
- Wektor: **512â€“768 dim**

ğŸ“¦ System trzyma DWIE bazy wektorowe:

| Index         | Jednostka wejÅ›ciowa         | Dim typowy | Kontekst wejÅ›ciowy     |
|---------------|----------------------------|------------|------------------------|
| ğŸ§± block_index | fragment (akapit / krok)    | 384        | ~512 tokenÃ³w           |
| ğŸ“„ note_index  | caÅ‚y plik `.md`             | 512â€“768    | 2K / 4K / 8K tokenÃ³w   |

> [!warning] ğŸŸ¥ Krytyczne
> âŒ Nie mieszamy 384-dim z 768-dim w jednym FAISS.  
> ğŸ” KaÅ¼dy indeks ma staÅ‚y wymiar.  
> ğŸ· KaÅ¼dy wektor zapisuje metadane:  
> `model_name`, `dim`, `context_window`, `index_type`.

---

### 0.2 âœ‚ Chunking

- âœ‚ Modele ~512 tokenÃ³w â†’ karmimy tylko fragmentami, nie caÅ‚ym plikiem.  
- ğŸ“ Modele 4K / 8K â†’ karmimy caÅ‚Ä… notatkÄ… `.md` bez ciÄ™cia.  
- ğŸª“ JeÅ›li dokument >8K tokenÃ³w â†’ dziel na logiczne sekcje (wejÅ›cie klienta / diagnoza / naprawa / wynik). Nie tnij losowo w poÅ‚owie zdania.

---

### 0.3 ğŸŒ JÄ™zyk PL / EN

> [!warning] ğŸŸª Jeden indeks PL+EN
> JeÅ›li chcesz jednego wektorowego mÃ³zgu i chcesz pytaÄ‡ po PL a dostaÄ‡ wynik z notatki EN:
>
> âœ… wybierasz:
> - ğŸŒ `EmbeddingGemma-300M`
> - â„ `snowflake-arctic-embed-m-v2.0`
>
> ğŸŸ¡ Modele EN-only (Arctic XS/S, Jina, Nomic) nie mapujÄ… poprawnie polskich opisÃ³w awarii.  
> JeÅ›li chcesz zostaÄ‡ przy EN-only â†’ zrÃ³b indeks EN osobno, PL osobno.

---

### 0.4 ğŸ–¥ SprzÄ™t globalnie

Definicje:
- `2t`, `4t`, `8t` = liczba logicznych wÄ…tkÃ³w CPU  
- AVX2 = nowoczesne SIMD w x86_64. Brak AVX2 = wolno

ğŸ§  CPU  
- wymagane: x86_64 z AVX2 albo ARM64 (Apple Silicon)  
- 2t wystarczy dla modeli lekkich  
- modele ~300M parametrÃ³w â†’ praktycznie chcÄ… 8t

ğŸ® GPU  
- ğŸ’š lekkie modele â†’ GPU niepotrzebna  
- ğŸ”´ ciÄ™Å¼kie modele (200+ MB fp16) â†’ GPU â‰¥6â€“8 GB VRAM drastycznie przyspiesza embedowanie setek plikÃ³w

ğŸ’¾ RAM hosta  
- ğŸŸ¢ lekkie (â‰¤70 MB fp16 wag): host 8 GB RAM ok  
- ğŸŸ  Å›rednie (70â€“220 MB fp16 wag): host 8â€“16 GB RAM  
- ğŸ”´ ciÄ™Å¼kie (â‰¥220 MB fp16 wag): host 16â€“32 GB RAM  
- â„¹ runtime = ~1.2â€“1.5Ã— wag fp16

---

## 1. âš¡ SMART CONNECTIONS
ğŸ’» Profil: Obsidian lokalnie, laptop/NUC/VM, brak mocnej GPU  
Cel: dziaÅ‚aÄ‡ ciÄ…gle bez zabijania hosta

PodziaÅ‚:
- 1A. ğŸ§± BLOKOWE (fragmenty â†’ block_index)
- 1B. ğŸ“„ LEKKIE NOTE (caÅ‚e pliki `.md` â†’ note_index light)

---

### 1A. ğŸ§± MODELE BLOKOWE  
(fragment tekstu, kontekst ~512 tokenÃ³w, dim ~384)

#### â„ snowflake-arctic-embed-xs  |  ğŸŸ¢ LEKKI / DOBRA JAKOÅšÄ†
model:: snowflake-arctic-embed-xs  
class:: smart-connections  
role:: block_index  
tier:: light  
dim:: 384  
context_tokens:: 512  
params_m:: 22  
lang:: EN  
multilang:: false  
ram_fp16_mb:: 42  
cpu_min_threads:: 2  
cpu_rec_threads:: 4  
gpu_required:: none  
host_ram_gb_min:: 8  
repo:: https://huggingface.co/Snowflake/snowflake-arctic-embed-xs  

ğŸ§¾ Opis  
â„ Bardzo lekki encoder-only.  
MaÅ‚y koszt, dobra trafnoÅ›Ä‡.  
To jest bazowy model block_index.

ğŸ¯ Zastosowanie  
- blokowe porÃ³wnanie akapitÃ³w  
- "czy to juÅ¼ naprawialiÅ›my"  
- szybkie linkowanie checklist i objawÃ³w awarii

Wniosek  
UstawiÄ‡ jako standard block_index. Stabilny.

---

#### â„ snowflake-arctic-embed-s  |  ğŸ”µ LEPSZA JAKOÅšÄ† / NADAL LEKKI
model:: snowflake-arctic-embed-s  
class:: smart-connections  
role:: block_index  
tier:: light  
dim:: 384  
context_tokens:: 512  
params_m:: 33  
lang:: EN  
multilang:: false  
ram_fp16_mb:: 63  
cpu_min_threads:: 2  
cpu_rec_threads:: 4  
gpu_required:: none  
host_ram_gb_min:: 8  
repo:: https://huggingface.co/Snowflake/snowflake-arctic-embed-s  

ğŸ§¾ Opis  
Wersja S rodziny â„ Arctic.  
WyÅ¼sza precyzja semantyczna niÅ¼ XS.

ğŸ¯ Zastosowanie  
- block_index kiedy trafnoÅ›Ä‡ > minimalny footprint

---

#### ğŸ” bge-micro-v2  |  ğŸŸ¡ ULTRA LOW RAM / AWARYJNIE
model:: bge-micro-v2  
class:: smart-connections  
role:: block_index  
tier:: light  
dim:: 384  
context_tokens:: 512  
params_m:: 17.4  
lang:: EN  
multilang:: false  
ram_fp16_mb:: 35  
cpu_min_threads:: 2  
cpu_rec_threads:: 4  
gpu_required:: none  
host_ram_gb_min:: 4  
repo:: https://huggingface.co/TaylorAI/bge-micro-v2  

ğŸ§¾ Opis  
Model zoptymalizowany pod maÅ‚y Å›lad pamiÄ™ci.  
Chodzi na sÅ‚abym laptopie / cienkiej VM.

ğŸ¯ Zastosowanie  
- block_index na hostach z bardzo maÅ‚Ä… iloÅ›ciÄ… RAM

Uwaga  
JakoÅ›Ä‡ gorsza niÅ¼ â„ Arctic XS/S.  
To jest "mam zÅ‚om, ale chcÄ™ AI".

---

#### ğŸ” bge-small (`bge--small`)  |  ğŸ”µ LEKKI STANDARD EN
model:: bge-small-en-v1.5  
class:: smart-connections  
role:: block_index  
tier:: light  
dim:: 384  
context_tokens:: 512  
params_m:: 33.4  
lang:: EN  
multilang:: false  
ram_fp16_mb:: 64  
cpu_min_threads:: 2  
cpu_rec_threads:: 4  
gpu_required:: none  
host_ram_gb_min:: 8  
repo:: https://huggingface.co/BAAI/bge-small-en-v1.5  

ğŸ§¾ Opis  
Klasyczny maÅ‚y encoder EN.  
W niektÃ³rych UI opisywany marketingowo jako `bge--small-4k`.  
Technicznie dalej ~512 tokenÃ³w, nie 4K.

ğŸ¯ Zastosowanie  
- block_index jeÅ›li nie chcesz rodziny Arctic

---

#### ğŸ§© gte-tiny  |  ğŸŸ  STARY KLASYK
model:: gte-tiny  
class:: smart-connections  
role:: block_index  
tier:: light  
dim:: 384  
context_tokens:: 512  
params_m:: 22.7  
lang:: EN  
multilang:: false  
ram_fp16_mb:: 43  
cpu_min_threads:: 2  
cpu_rec_threads:: 4  
gpu_required:: none  
host_ram_gb_min:: 8  
repo:: https://huggingface.co/TaylorAI/gte-tiny  

ğŸ§¾ Opis  
Historyczny tani similarity search.

ğŸ¯ Zastosowanie  
- block_index

Status  
ZastÄ™powany praktycznie przez â„ Arctic XS bo Arctic XS daje lepszy quality/cost.

---

#### ğŸ§© Ivysaur  |  ğŸŸ£ AUTOCOMPLETE / NOT SEARCH
model:: Ivysaur  
class:: smart-connections  
role:: writing-assist  
tier:: light  
dim:: 384  
context_tokens:: 512  
params_m:: 22.7  
lang:: EN  
multilang:: false  
ram_fp16_mb:: 43  
cpu_min_threads:: 2  
cpu_rec_threads:: 4  
gpu_required:: none  
host_ram_gb_min:: 8  
repo:: https://huggingface.co/Mihaiii/Ivysaur  

ğŸ§¾ Opis  
To nie jest retrieval wiedzy.  
To jest semantyczne "dokoÅ„cz myÅ›l".

ğŸ¯ Zastosowanie  
- âœ wspomaganie pisania  
- NIE do block_index / note_index

---

### 1B. ğŸ“„ MODELE NOTATKOWE LEKKIE  
(caÅ‚e pliki `.md`, kontekst 2Kâ€“8K, dim ~512)

#### ğŸ“„ jina-embeddings-v2-small-en (2K / 4K / 8K)  |  ğŸ’™ NOTE INDEX LOKALNY
model:: jina-embeddings-v2-small-en  
class:: smart-connections  
role:: note_index  
tier:: medium  
dim:: 512  
context_tokens:: 8000  
params_m:: 33  
lang:: EN  
multilang:: false  
ram_fp16_mb:: 63  
cpu_min_threads:: 4  
cpu_rec_threads:: 8  
gpu_required:: optional  
host_ram_gb_min:: 8  
repo:: https://huggingface.co/jinaai/jina-embeddings-v2-small-en  

ğŸ§¾ Opis  
Encoder-only (JinaBERT).  
Warianty 2K / 4K / 8K.  
Wersje 4K / 8K â†’ caÅ‚a notatka `.md` jako jeden embedding bez ciÄ™cia.

ğŸ¯ Zastosowanie  
- note_index lokalny  
- "pokaÅ¼ mi inne notatki podobne do tej notatki"

WdroÅ¼enie praktyczne (profil laptop / Obsidian):
- ğŸ§± block_index â†’ â„ Arctic XS  
- ğŸ“„ note_index â†’ ğŸ“„ Jina Small (4K/8K)  
= stabilny Smart Connections offline

---

## 2. ğŸš€ HIGH PERFORMANCE
ğŸ­ Centralna baza wiedzy.  
ğŸ“ DÅ‚ugi kontekst (8K).  
ğŸŒ Opcjonalnie PL+EN.  
ğŸ® Wymaga mocniejszej maszyny.

Te modele poniÅ¼ej: ğŸ“„ `note_index` high-end.

---

#### ğŸš€ jina-embeddings-v2-base-en (8K)  |  ğŸ”¥ DÅUGIE DOKUMENTY EN
model:: jina-embeddings-v2-base-en  
class:: high-performance  
role:: note_index  
tier:: medium  
dim:: 768  
context_tokens:: 8000  
params_m:: 137  
lang:: EN  
multilang:: false  
ram_fp16_mb:: 261  
cpu_min_threads:: 4  
cpu_rec_threads:: 8  
gpu_required:: recommended  
host_ram_gb_min:: 16  
repo:: https://huggingface.co/jinaai/jina-embeddings-v2-base-en  

ğŸ§¾ Opis  
WiÄ™kszy JinaBERT.  
Czyta ~8K tokenÃ³w w jednym przebiegu.  
Nadaje siÄ™ do dÅ‚ugich raportÃ³w technicznych po angielsku.

ğŸ¯ Zastosowanie  
- note_index high-end (EN)  
- raporty incydentÃ³w, audyty, peÅ‚ne SOP bez chunkingu

---

#### â„ snowflake-arctic-embed-m  |  ğŸŸ¦ EN / 2K
model:: snowflake-arctic-embed-m  
class:: high-performance  
role:: note_index  
tier:: medium  
dim:: 768  
context_tokens:: 2000  
params_m:: 110  
lang:: EN  
multilang:: false  
ram_fp16_mb:: 210  
cpu_min_threads:: 4  
cpu_rec_threads:: 8  
gpu_required:: optional  
host_ram_gb_min:: 16  
repo:: https://huggingface.co/Snowflake/snowflake-arctic-embed-m  

ğŸ§¾ Opis  
â„ Arctic M. KrÃ³tszy kontekst (~2K), wysoka jakoÅ›Ä‡ EN.

ğŸ¯ Zastosowanie  
- note_index dla Å›redniej dÅ‚ugoÅ›ci dokumentÃ³w EN  
- kiedy chcesz jakoÅ›Ä‡ Arctic, ale nie potrzebujesz peÅ‚nych 8K

---

#### â„ snowflake-arctic-embed-m-long  |  ğŸ”µ EN / 8K
model:: snowflake-arctic-embed-m-long  
class:: high-performance  
role:: note_index  
tier:: medium  
dim:: 768  
context_tokens:: 8000  
params_m:: 137  
lang:: EN  
multilang:: false  
ram_fp16_mb:: 261  
cpu_min_threads:: 4  
cpu_rec_threads:: 8  
gpu_required:: optional  
host_ram_gb_min:: 16  
repo:: https://huggingface.co/Snowflake/snowflake-arctic-embed-m  

ğŸ§¾ Opis  
Wariant long.  
Do ~8K tokenÃ³w EN na raz.  
Odpowiednik `m`, ale dla dÅ‚ugich dokumentÃ³w.

ğŸ¯ Zastosowanie  
- note_index high-end (EN) z dÅ‚ugim kontekstem  
- peÅ‚ne SOP / audyty / incydenty bez dzielenia

---

#### â„ snowflake-arctic-embed-m-v2.0  |  ğŸŸª PL+EN / 8K / GRUBY
model:: snowflake-arctic-embed-m-v2.0  
class:: high-performance  
role:: note_index  
tier:: heavy  
dim:: 768-1024  
context_tokens:: 8000  
params_m:: 305  
lang:: PL+EN  
multilang:: true  
ram_fp16_mb:: 600  
cpu_min_threads:: 8  
cpu_rec_threads:: 8+  
gpu_required:: required  
host_ram_gb_min:: 24  
repo:: https://huggingface.co/Snowflake/snowflake-arctic-embed-m-v2.0  

ğŸ§¾ Opis  
â„ Arctic M v2.0.  
8K kontekstu.  
Multilang (PL+EN).  
CiÄ™Å¼kie ~600 MB fp16 (~800 MB runtime).

ğŸ¯ Zastosowanie  
- note_index high-end dla PL+EN  
- "jeden mÃ³zg firmy" â†’ jeden wspÃ³lny indeks wiedzy  
- brak chunkingu do ~8K tokenÃ³w

Wniosek  
To jest docelowy silnik kiedy firma potrzebuje *jednego* indeksu PL+EN z peÅ‚nym kontekstem.

---

#### ğŸ§¬ nomic-embed-text-v1.5  |  ğŸ§¬ RAG / MAÅY INDEKS
model:: nomic-embed-text-v1.5  
class:: high-performance  
role:: note_index  
tier:: medium  
dim:: 768 (â†“512 â†“256 â†“128 â†“64)  
context_tokens:: 8000  
params_m:: 100-138  
lang:: EN  
multilang:: false  
ram_fp16_mb:: 191  
cpu_min_threads:: 4  
cpu_rec_threads:: 8  
gpu_required:: recommended  
host_ram_gb_min:: 8  
repo:: https://huggingface.co/nomic-ai/nomic-embed-text-v1.5  

ğŸ§¾ Opis  
MRL (Matryoshka).  
MoÅ¼na ciÄ…Ä‡ wymiar wektora z 768 â†’ 256 â†’ 128 â†’ 64 przy maÅ‚ej utracie jakoÅ›ci.  
To zmniejsza koszt magazynu wektorowego.

ğŸ¯ Zastosowanie  
- duÅ¼y RAG (setki / tysiÄ…ce dokumentÃ³w EN)  
- gdy rozmiar indeksu musi byÄ‡ tani (np. FAISS w RAM edge)

---

#### ğŸŒ embeddinggemma-300m  |  ğŸŸª PL+EN / 2K / LEPIEJ RAM NIÅ» m-v2.0
model:: embeddinggemma-300m  
class:: high-performance  
role:: note_index  
tier:: heavy  
dim:: 768  
context_tokens:: 2000  
params_m:: 300  
lang:: PL+EN  
multilang:: true  
ram_fp16_mb:: 572  
cpu_min_threads:: 8  
cpu_rec_threads:: 8+  
gpu_required:: recommended  
host_ram_gb_min:: 16  
repo:: https://huggingface.co/google/embeddinggemma-300m  

ğŸ§¾ Opis  
ğŸŒ Encoder-only Gemma.  
Rozumie PL i EN w jednym modelu.  
DziaÅ‚a offline.  
WspÃ³lny indeks PL+EN bez rozdzielania.

ğŸ¯ Zastosowanie  
- note_index wielojÄ™zyczny przy krÃ³tszych dokumentach (2K kontekstu)  
- zapytanie PL â†’ trafienie EN  
- zapytanie EN â†’ trafienie PL

Ograniczenie  
Kontekst tylko ~2K.  
DÅ‚ugie SOP PL/EN trzeba ciÄ…Ä‡.  
JeÅ›li chcesz PL+EN i dÅ‚ugie 8K kontekstu â†’ â„ `snowflake-arctic-embed-m-v2.0`.

---

## 3. ğŸ” PRESETY WDROÅ»ENIOWE

### 3.1 ğŸ’» Obsidian lokalnie (laptop / NUC / brak GPU)
preset:: obsidian-local  
class:: preset  
block_index_model:: snowflake-arctic-embed-xs  
note_index_model:: jina-embeddings-v2-small-en  
lang_scope:: EN  
multilang_single_index:: false  
gpu_needed:: false  
min_ram_gb:: 8  

SkÅ‚ad:
- ğŸ§± block_index â†’ â„ `snowflake-arctic-embed-xs` (384-dim, ~42 MB fp16, EN)  
- ğŸ“„ note_index â†’ ğŸ“„ `jina-embeddings-v2-small-en` (4K / 8K, ~512-dim, ~63 MB fp16, EN)

Wymagania:
- CPU: 4t ok  
- GPU: brak wymogu  
- RAM hosta: 8â€“16 GB

Use case:
- auto-link SOP / checklist / intake w Obsidianie
- codzienna praca offline

---

### 3.2 ğŸ”¥ DÅ‚ugie SOP po angielsku (8K kontekstu)
preset:: long-en-docs  
class:: preset  
block_index_model:: snowflake-arctic-embed-s  
note_index_model:: jina-embeddings-v2-base-en  
lang_scope:: EN  
multilang_single_index:: false  
gpu_needed:: true  
min_ram_gb:: 16  

SkÅ‚ad:
- ğŸ§± block_index â†’ â„ `snowflake-arctic-embed-s` (384-dim)  
- ğŸ“„ note_index â†’ ğŸš€ `jina-embeddings-v2-base-en` (~768-dim, 8K kontekst, ~261 MB fp16)

Wymagania:
- CPU: 8t zalecane  
- GPU: â‰¥6â€“8 GB VRAM zalecana przy batchowym embedowaniu  
- RAM hosta: â‰¥16 GB

Use case:
- audyty, incydenty, raporty po EN
- "pokaÅ¼ podobny incydent" bez chunkowania dÅ‚ugich plikÃ³w

---

### 3.3 ğŸŒ Jeden mÃ³zg PL+EN
preset:: pl-en-shared-brain  
class:: preset  
block_index_model:: snowflake-arctic-embed-s (opcjonalnie tylko EN bloki)  
note_index_model:: snowflake-arctic-embed-m-v2.0 / embeddinggemma-300m  
lang_scope:: PL+EN  
multilang_single_index:: true  
gpu_needed:: true  
min_ram_gb:: 24  

Wariant ğŸŸ¡ lÅ¼ejszy (mniej RAM, krÃ³tszy kontekst):
- ğŸŒ `EmbeddingGemma-300M`  
  - dim ~768  
  - kontekst ~2K  
  - ~572 MB fp16  
  - PL+EN

Wariant ğŸ”´ ciÄ™Å¼szy (wiÄ™cej RAM, peÅ‚ne 8K kontekstu):
- â„ `snowflake-arctic-embed-m-v2.0`  
  - dim ~768â€“1024  
  - kontekst ~8K  
  - ~600 MB fp16  
  - PL+EN

Wymagania:
- CPU: 8t+  
- GPU: karta â‰¥8 GB VRAM zalecana  
- RAM hosta:
  - Gemma: â‰¥16â€“32 GB  
  - m-v2.0: â‰¥24â€“32 GB

Use case:
- centralny indeks wiedzy warsztatu
- PL i EN razem, jedno zapytanie

---

## 4. ğŸ” DATAVIEW / ZAPYTANIA

### 4.1 Wszystkie modele embeddingÃ³w ze sprzÄ™tem
```dataview
TABLE model, class, role, tier, dim, context_tokens, params_m, lang, multilang, ram_fp16_mb, host_ram_gb_min, cpu_min_threads, gpu_required
FROM "."
WHERE module = "embeddings" AND dv_ready = true AND model
SORT class ASC, tier ASC, ram_fp16_mb ASC
```

### 4.2 Modele do block_index (fragmenty)
```dataview
TABLE model, tier, dim, context_tokens, ram_fp16_mb, cpu_min_threads, gpu_required
FROM "."
WHERE role = "block_index"
SORT ram_fp16_mb ASC
```

### 4.3 Modele do note_index (caÅ‚e pliki `.md`)
```dataview
TABLE model, tier, lang, multilang, context_tokens, dim, ram_fp16_mb, gpu_required
FROM "."
WHERE role = "note_index"
SORT multilang DESC, context_tokens DESC
```

### 4.4 Tylko modele PL+EN
```dataview
TABLE model, context_tokens, ram_fp16_mb, host_ram_gb_min, gpu_required
FROM "."
WHERE multilang = true AND role = "note_index"
SORT context_tokens DESC
```

### 4.5 Presety wdroÅ¼eniowe
```dataview
TABLE preset, block_index_model, note_index_model, lang_scope, multilang_single_index, min_ram_gb, gpu_needed
FROM "."
WHERE class = "preset"
SORT min_ram_gb ASC
```

---

## 5. ğŸ”— REPOZYTORIA (waga modeli do pobrania)

- â„ snowflake-arctic-embed-xs  
  https://huggingface.co/Snowflake/snowflake-arctic-embed-xs  
- â„ snowflake-arctic-embed-s  
  https://huggingface.co/Snowflake/snowflake-arctic-embed-s  
- â„ snowflake-arctic-embed-m  
  https://huggingface.co/Snowflake/snowflake-arctic-embed-m  
- â„ snowflake-arctic-embed-m-v2.0  
  https://huggingface.co/Snowflake/snowflake-arctic-embed-m-v2.0  
- ğŸ“„ jina-embeddings-v2-small-en  
  https://huggingface.co/jinaai/jina-embeddings-v2-small-en  
- ğŸš€ jina-embeddings-v2-base-en  
  https://huggingface.co/jinaai/jina-embeddings-v2-base-en  
- ğŸ§¬ nomic-embed-text-v1.5  
  https://huggingface.co/nomic-ai/nomic-embed-text-v1.5  
- ğŸŒ embeddinggemma-300m  
  https://huggingface.co/google/embeddinggemma-300m  
- ğŸ” bge-micro-v2  
  https://huggingface.co/TaylorAI/bge-micro-v2  
- ğŸ” bge-small-en-v1.5  
  https://huggingface.co/BAAI/bge-small-en-v1.5  
- ğŸ§© gte-tiny  
  https://huggingface.co/TaylorAI/gte-tiny  
- ğŸ§© Ivysaur  
  https://huggingface.co/Mihaiii/Ivysaur  

---

## 6. ğŸ“¦ POBIERANIE WAG (`git lfs clone`)

Upewnij siÄ™ Å¼e masz git-lfs:
```bash
sudo apt-get update && sudo apt-get install git-lfs -y
git lfs install
```

ğŸ§± Block index (fragmenty / akapity)
```bash
git lfs clone https://huggingface.co/Snowflake/snowflake-arctic-embed-xs
git lfs clone https://huggingface.co/Snowflake/snowflake-arctic-embed-s
git lfs clone https://huggingface.co/TaylorAI/bge-micro-v2
git lfs clone https://huggingface.co/BAAI/bge-small-en-v1.5
git lfs clone https://huggingface.co/TaylorAI/gte-tiny
git lfs clone https://huggingface.co/Mihaiii/Ivysaur
```

ğŸ“„ Note index (caÅ‚e pliki `.md`, lokalnie / Obsidian)
```bash
git lfs clone https://huggingface.co/jinaai/jina-embeddings-v2-small-en
```

ğŸš€ Note index high-end (8K kontekstu, baza wiedzy)
```bash
git lfs clone https://huggingface.co/jinaai/jina-embeddings-v2-base-en
git lfs clone https://huggingface.co/Snowflake/snowflake-arctic-embed-m
git lfs clone https://huggingface.co/Snowflake/snowflake-arctic-embed-m-v2.0
git lfs clone https://huggingface.co/nomic-ai/nomic-embed-text-v1.5
git lfs clone https://huggingface.co/google/embeddinggemma-300m
```

## ğŸ”— Backlinks

- [[jakubc]]
- [[INDEX]]
- [[core]]

---
*Auto-generated backlinks for cluster connectivity*
